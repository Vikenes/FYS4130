\documentclass[a4paper, english, 12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel, textcomp, color, amssymb, subfig, float}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{multicol}   
\usepackage{bm}
\usepackage{gensymb}
\usepackage{amsmath}


\usepackage{physics}
\usepackage{tikz}
\usepackage{pgfplots}
\newcommand{\eps}{\epsilon}
\newcommand{\closed}[1]{\left( #1 \right)}
\newcommand{\bracket}[1]{\left[ #1 \right]}
\newcommand{\curlybig}[1]{\left\{ #1 \right\} }
\newcommand{\curly}[1]{\{ #1 \} }
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\Z}{\mathcal{Z}}

%\newcommand{\addPLOT}[4]{
%\addplot [domain=#1:#2,samples=200,color=#3,]{#4};}
%\newcommand{\addCOORDS}[1]{\addplot coordinates {#1};}
%\newcommand{\addDRAW}[1]{\draw #1;}
%\newcommand{\addNODE}[2]{ \node at (#1) {#2};}

%		\PLOTS{x}{y}{left}{
%			\ADDPLOT{x^2}{-2}{2}{blue}
%			\ADDCOORDS{(0,1)(1,1)(1,2)}
%		}


\definecolor{svar}{RGB}{0,0,0}
\definecolor{opgavetekst}{RGB}{109,109,109}
\definecolor{blygraa}{RGB}{44,52,59}

\hoffset = -60pt
\voffset = -95pt
\oddsidemargin = 0pt
\topmargin = 0pt
\textheight = 0.97 \paperheight
\textwidth = 0.97 \paperwidth

\begin{document}
\tiny
\begin{multicols*}{2}


\subsection*{\boxed{\text{Thermodynamic postulates}}}

\begin{enumerate}
    \item \textbf{Equilibrium states:} There exists equilibrium states of a macroscopic system that are characterized by a small number of extensive variables. 
    \item \textbf{Entropy maximization:} The values assumed by the extensive parameters of an isolated composite system in the absence of an internal constraint are those that maximize the entropy over the set of all constrained macroscopic states. $(\Delta S \geq 0)$
    \item \textbf{Additivity:} The entropy of a composite system is additive over the constituent subsystems $(S(E_a)+S(E_b) = S(E_a+E_b))$
    \item \textbf{Continuity and differentiability:} The entropy is a continuous and differentiable function of the extensive parameters. 
    \item \textbf{Extensivity:} The entropy is an extensive function of the extensive variables (not the case when boundary effects are important). $(S(\lambda E, \lambda V)=\lambda S(E,V))$
    \item \textbf{Monotonicity:} The entropy is a monotonically increasing function of the energy for equilibrium values of the energy. 
    \item \textbf{The Nernst postulate:} The entropy of any (real/quantum) system is non-negative. ($\lim_{T\to0} S(T)\geq0$)
\end{enumerate}


\subsubsection*{\boxed{\text{Mathematical identities}}}
\textbf{Stirling's approximation}
\begin{align*}
  N! &\approx N^Ne^{-N}\quad(\cdot \sqrt{2\pi N}) \\
  \ln{(N!)} &\approx N\ln{N}-N \\
\end{align*}

\textbf{Dirac delta-function}

\begin{align*}
    \int dx\,\delta(u-ax) &= \frac{1}{a}\int dx\,\delta(u/a - x) \\ 
    \int f(x)\delta(g(x))\,dx &= \sum_j \frac{f(x_j)}{g'(x_j)}
\end{align*}

\textbf{Geometric series}
\begin{align*}
    \sum_{n=0}^\infty x^n = \frac{1}{1-x}
\end{align*}

\textbf{Limit}
\begin{align*}
    \lim_{x\to0} x\ln x = 0
\end{align*}

\textbf{The Euler Equation:}
For an extensive system we have 
\begin{align*}
    U=TS-PV+\mu N
\end{align*}

\textbf{Gibbs-Duhem:}
\begin{align*}
    dU=0 &= SdT-VdP+Nd\mu \\ 
    \implies d\mu &= -(S/N)dT+(V/N)dP  
\end{align*}


\subsubsection*{\scriptsize Standard set of second derivatives}
\begin{align*}
    \alpha &= \frac{1}{V}\left(\pdv{V}{T} \right)_{P,N}, \text{thermal expansion} \\ 
    \kappa_T &= -\frac{1}{V}\left(\pdv{V}{P} \right)_{T,N}, \text{isothermal compressibility} \\ 
    c_P &= \frac{T}{N}\left(\pdv{S}{T} \right)_{P,N}, \text{spec. heat pr. part. at const. pressure} \\ 
    c_V &= \frac{T}{N}\left(\pdv{S}{T} \right)_{V,N}, \text{spec. heat pr. part. at const. volume}
\end{align*}

Relations between some of them are:
\begin{align*}
    c_P &= c_V + \frac{\alpha^2 TV}{N\kappa_T} \\ 
    \kappa_S &= \kappa_T - \frac{TV \alpha^2}{Nc_P}
\end{align*}


\subsubsection*{\scriptsize Extremum principles}
Consider the entropy as a function of internal extensive parameters, $U$ and $X$. Entropy as maximized in equilibrium, $\pdv{S}{X}=0,\,\pdv[2]{S}{X}<0$, and expanding around this maxima we get 
\begin{align*}
    dS &= \pdv{S}{U} dU + \pdv{S}{X}dX + \pdv[2]{S}{X}dX^2 = \frac{dU}{T} + \pdv[2]{S}{X}dX^2 \\ 
    dU &= TdS - T \pdv[2]{S}{X}dX^2 
\end{align*}
For a process with $dS=0$ (quasi-static), the second derivaitve of the entropy is negative, and we get $\pdv[2]{U}{X}>0$, so \textit{energy is minimized} in equilibrium.  

\textbf{Helmholtz free energy:} Total energy of reservoir and system is $U+U_R$, with its first and second partial derivative wrt $X$ being $0$ and $>0$, respectively, in equilibruim when entropy, volume and number of particles is also given as the sum of that of the system and reservoir. The entropy change, and subsequent change in $F$ becomes 
\begin{align*}
    \pdv{(S+S_R)}{X} &= 0 \implies \pdv{S}{X} = - \dv{S_R}{X} \\ 
    \pdv{F}{X} &= \pdv{U-TS}{X} = \pdv{U}{X} + T\pdv{S_R}{X} = \pdv{U}{X} + \pdv{U_R}{X}
\end{align*}
Taking the second order derivative we get 
\begin{align*}
    \pdv[2]{F}{X} &= \pdv[2]{U}{X} - T \pdv[2]{S}{X} = \pdv[2]{U+U_R}{X} > 0
\end{align*} 
So the Helmholt'z free energy is also minimized in equilibrium. Similar analysis holds for Enhtalpy, with energy minima subject to constant volume, $\partial/\partial X (V+V_R)=0$. 


\textbf{Experiments related to these extremum principles:}
\begin{itemize}
    \item Entropy: Thermally isolated system with two subvolumes divided by a moveable piston. Releasing the piston, it moves such that it maximizes the entropy
    \item Energy: Piston now connected by a rod to something outside the cyllinder. Piston is moved quasi-statically to a position with no net force due to the gas. 
    \item Helmholtz Free Energy: Same as with the energy experiment, but with cyllinder placed in a thermal reservoir of termperature $T$, with which it can exchange energy and entropy. 
    \item Enthalpy: System in contact with a reservoir at constant pressure that can compress the system.   
    \item Gibbs Free Energy: System in contact with both a pressure reservoir and a temperature reservoir.   
\end{itemize}


\subsubsection*{\scriptsize Stability Conditions}
Will now consider what conditions a subsystem must satisfy as a consequence of extremum principles. Consider two combined systems, $S,U,V,N$ and $\hat{S},\hat{U},\hat{V},\hat{N}$, isolated from the rest of the universe. For total constant volume at equilibrium, total energy is minimized. We move a piston such that the two volumeschange by $\Delta V$. The total change in energy is then the change in energy of each subsystem 
\begin{align*}
    \Delta U_{tot} = U(S,V+\Delta V,N) + \hat{U}(\hat{S},\hat{V}-\Delta V,\hat{N}) - [U(S,V,N) + \hat{U}(\hat{S},\hat{V},\hat{N})] \geq 0
\end{align*} 
Now, consider the two subsystems to be identical, such that the last term becomes $-2U(S,V,N)$. Dividing both sides by $(\Delta V)^2$ and taking the limit $\Delta V\to0$, we obtain a condition in terms of the second derivative of energy wrt volume, which gives us two stability conditions. 
\begin{align*}
    \closed{\pdv[2]{U}{V}}_{S,N} = -\closed{\pdv{P}{V}}_{S,N} = \frac{1}{V\kappa_S} =  \geq0 
\end{align*} 
So we must have $\kappa_S \geq0$

Performing the same analysis once again, but with an entropy change, $\Delta S$ between the two identical subsystems, we get the stability condition 
\begin{align*}
    \closed{\pdv[2]{U}{S}}_{V,N} = \closed{\pdv{T}{S}}_{V,N} = \frac{T}{N c_V} \geq 0 
\end{align*}
Alternatively, one may consider the change in entropy from exchange in energy, which gives 
\begin{align*}
    \closed{\pdv[2]{S}{U}}_{V,N} = -\frac{1}{T^2} \closed{\pdv{T}{U}}_{V,N} = - \frac{1}{N T^2 c_V} \leq 0
\end{align*} 
Both cases give the condition that $c_V\geq0$. 

For volume exchange in the Helmholtz free energy, we get 
\begin{align*}
    \closed{\pdv[2]{F}{V}}_{T,N} = -\closed{\pdv{P}{V}}_{T,N} = \frac{1}{V \kappa_T} \geq 0 
\end{align*}
so that $\kappa_T \geq 0$. 

For an intensive quantity, such as $T$, one finds that 
\begin{align*}
    \closed{\pdv[2]{F}{T}}_{V,N} = -\closed{\pdv{S}{T}}_{V,N} = -\frac{N}{T}c_V \leq 0 
\end{align*} 
since $c_V\geq0$. 

For e.g. $F$ and $U$, the Legendre transform gives us the following relation 
\begin{align*}
    \closed{\pdv[2]{F}{T}}_{V,N} = - 1 \Big/ \closed{\pdv[2]{U}{S}}_{V,N} 
\end{align*}

In general, the second partial derivative of a quantity ($U$) with respect to a variable ($S$) has opposite sign of the second partial derivative of the Legende transform ($F$) with respect to the new variable ($T$). It is also a general rule that the two second partial derivatives are negative reciprocals of each other.     

As a summary, the stability conditions are 
\begin{align*}
    U(S,V,N):&\quad \closed{\pdv[2]{U}{S}}_{V,N} \geq0,\quad \closed{\pdv[2]{U}{V}}_{S,N} \geq 0 \\ 
    F(T,V,N):&\quad \closed{\pdv[2]{F}{T}}_{V,N} \leq0,\quad \closed{\pdv[2]{F}{V}}_{T,N} \geq 0 \\
    H(S,P,N):&\quad \closed{\pdv[2]{H}{S}}_{P,N} \geq0,\quad \closed{\pdv[2]{H}{P}}_{S,N} \leq 0 \\ 
    G(T,P,N):&\quad \closed{\pdv[2]{G}{T}}_{P,N} \leq0,\quad \closed{\pdv[2]{G}{P}}_{T,N} \leq 0 
\end{align*}



\subsubsection*{\scriptsize Phase Transitions}
To illustrate, consider the van der Waals Fluid. From ideal gas, add attraction term, $-a N^2/V$, for neighboring particles, and restrict volume due to hard particle spheres, $V\to V-bN$. This yields 
\begin{align*}
    F_{IG} &= -N k_B T \bracket{\ln(V/N) + \frac{3}{2}\ln(k_B T)+X} \\ 
    F_{VdW} &= - N k_B T \bracket{\ln\closed{\frac{V-bN}{N}} + \frac{3}{2}\ln(k_B T)+X} - a(N^2/V)
\end{align*}

This yields the following expression for pressure and energy 
\begin{align*}
    P = \frac{Nk_B T}{V - bN} - \frac{a N^2}{V^2},\quad U = \frac{3}{2}N k_B T - a \closed{\frac{N^2}{V}}
\end{align*}


PT 
BOLTZMANN+MAX-BOLT 

\subsection*{\boxed{\text{Classical statistical mechanics}}}

The microcanonical ensemble is defined by assigning equal probability to each microstate, so within a given energy range, the probability is $P_s=1/W$, where $W$ is the number of microstates in the energy range.   

\begin{align*}
    \Omega(E,V,N) &=\frac{1}{h^{3N}N!}\int dq \int dp\, \delta(E-H(p,q)) \\
    Z &= \int dE\, \Omega(E,V,N) \exp(-\beta E) \\
    &= \frac{1}{h^{3N}N!}\int dq \int dp\, e^{-\beta H(q,p)} 
\end{align*}


\subsubsection*{\scriptsize Liouville Theorem}
With systems near a region in phase space (ensemble), we can talk about a density of systems on a grained scale $h^{3N}$. The number of systems in the region $(\Q,\P)$ at a time $t$ is then $\rho(\Q,\P,t)d\Q d\P$. In equilibrium the macroscopic quantities should be independent of time. So for equilibrium states, the density should \textit{not} change in time. The system will follow Newton's e.o.m., so it obeys the continuity equation 
\begin{align*}
    \pdv{\rho}{t} &= -\vec{\nabla} \cdot (\rho \vec{v}),\quad \vec{\nabla}=\closed{\pdv{\Q},\pdv{\P}},\: \vec{v}=\closed{\dot{\Q},\dot{\P}} \\ 
    \pdv{\rho}{t} &= -\sum_{\alpha=1}^{3N}\closed{\pdv{(\rho\dot{q}_\alpha)}{q_\alpha} + \pdv{(\rho\dot{p}_\alpha)}{p_\alpha} }
\end{align*} 
Using the product rule for differentiation, Hamilton's equations $\dot{q}_\alpha=\pdv{H}{p_\alpha},\,\dot{p}_\alpha=-\pdv{H}{q_\alpha}$ make the derivatives of the the dotted terms cancel each other, and we get 
\begin{align*}
    \pdv{\rho}{t} &= -\sum_{\alpha=1}^{3N} \closed{\pdv{\rho}{q_\alpha} \dot{q}_\alpha + \pdv{\rho}{p_\alpha}\dot{p}_\alpha }
\end{align*}  
The change in the density as we flow with the system is now found by the total time derivative of the density 
\begin{align*}
    \dv{\rho}{t} &= \pdv{\rho}{t} + \sum_{\alpha=1}^{3N} \closed{\pdv{\rho}{q_\alpha} \dot{q}_\alpha + \pdv{\rho}{p_\alpha}\dot{p}_\alpha } = 0
\end{align*}
This is the Liouville theorem. There is thus no local change in the density following a volume element, and the flow in phase space is incompressible. However, in equilibrium statistical mechanics, $\rho$ itself should be $t$ independent in order for measurable quantities to be $t$ independent as well. That is, we want $\pdv{\rho}{t}=0$. Using Poisson brackets, we then have 
\begin{align*}
    \pdv{\rho}{t} = \curly{H,\rho},\quad \curly{A,B}=\sum_\alpha \closed{\pdv{A}{q_\alpha}\pdv{B}{p_\alpha} - \pdv{A}{p_\alpha}\pdv{B}{q_\alpha}} 
\end{align*}

If $\curly{H,\rho}=0$, we get the desired equilibrium result $\pdv{\rho}{t}=0$. If $\rho=\rho(H)$, the chain rule of differentiation will immideately yield $\curly{H,\rho(H)}=0$. This means that the microcanonical ensemble is constant in time, and only depends on $H$. 
\begin{align*}
    \rho(\Q,\P) = \frac{1}{h^{3N} N!} \delta\closed{E-H(\Q,\P)}
\end{align*}


\subsubsection*{\scriptsize The ergodic hypothesis}
In equilibrium statistical mechanics we have ensemble averages $\expval{f}$. Experimentally, however, we usually measure a single system over some finite time, with the average $\bar{f}=\frac{1}{T} \int_0^T dt\,f(T)$. The ergodic hypothesis states that $\boxed{\expval{f}=\bar{f}}$. Can be extremely hard to prove, but holds whenever the trajectories of (nearly) every point in phase-space eventually passes close to every other point on the surface of constant energy. In practice one is often satisfied with $\expval{f}=\bar{f}+\eps$. 

We do have $\expval{f}\neq\bar{f}$ for:
\begin{itemize}
    \item Few-body problem with chaos 
    \item Integrable systems, many-body systems with an infinite number of conserved quantities (typically 1D)
    \item Borken symmetry phases
    \item Glasses, very slow dynamics
\end{itemize} 


\subsection*{\boxed{\text{QM statistical mechanics}}}

\begin{align*}
    P_n &= \frac{e^{-\beta E_n}}{Z} \\
    Z &= \sum_l \Omega(l)\exp(-\beta E_l),\quad \Omega(l)=\text{Degeneracy of energy level}\: l
\end{align*}


\begin{align*}
    \closed{\pdv{(\beta F)}{\beta}}_{V,N} = -\pdv{\ln Z}{\beta} \implies \beta F = -\ln Z + f(V,N)
\end{align*}
For identical particles $f(V,N)=0$, for distinguishable particles $f(V,N)=-\ln N!$. 

For the entropy, we have 
\begin{align*}
    S&=-\closed{\pdv{F}{T}}_{V,N} = k_B \ln Z + \frac{1}{T}\expval{E} = k_B \ln Z + \frac{1}{T}\sum_n E_n P_n \\ 
    &= k_B \sum_n \closed{P_n \ln Z + \beta E_n P_n} = k_B \sum_n \closed{P_n \ln Z - \ln(ZP_n) P_n}
\end{align*}


\begin{align*}
    S = -k_B \sum_n P_n \ln P_n
\end{align*}

\subsubsection*{\scriptsize Third law of TD}
Let $l=0$ be the lowest energy level. The partition function can be written as 
\begin{align*}
    Z = \Omega(0)e^{-\beta E_0} \closed{1 + \sum_{l>0} \frac{\Omega(l)}{\Omega(0)} e^{-\beta(E_l-E_0)} }    
\end{align*}
Since $E_l>E_0$, for $T\to0$ the sum dies, and we get $Z \to \Omega(0) e^{-\beta E_0}$ for $T\to0$. The probability of finding the system in one of the ground states is then 
\begin{align*}
    P_0 = \frac{e^{-\beta E_0}}{\Omega(0)e^{-\beta E_0}} = \frac{1}{\Omega(0)}
\end{align*}
So it is equally distributed among the $\Omega(0)$ states. For $T\to0$ we also have $P_n\to0$. Using the entropy definition then gives 
\begin{align*}
    S = -k_B \sum_n P_n \ln P_n = k_B \Omega(0) \closed{\frac{1}{\Omega(0)}\ln(\Omega(0))} = k_B \ln\Omega(0)
\end{align*}
So for $T\to0$ the entropy is independent of $T$, and $\Omega(0)\geq1$ since it is a degeneracy, which means that $S(T\to0)\geq0$. Even for $\Omega(0)=a^N$, that is the degeneracy depends on the system size and $a>1$, we have $S(T=0)/N=k_B \ln a>0$. 

\begin{align*}
    E_n &= \sum_{j=1}^N E_{n_j} \implies e^{-\beta E_n} = \prod_{j=1}^N e^{-\beta E_{n_j}} \\
    Z &= \sum_{\curly{n_j}} \prod_{j=1}^N \exp(-\beta E_{n_j}) = \prod_{j=1}^N \left( \sum_{n_j} \exp(-\beta E_{n_j}) \right)
\end{align*}

\begin{align*}
    \epsilon_{\vec{k}} &= \frac{\hbar^2 }{2m}k^2 = \frac{\hbar^2 \pi^2}{2mL^2}n^2 = \epsilon_{\vec{k}} \\ 
    D(\epsilon) &= \int_0^\infty dn_x\,dn_y\,dn_z \delta(\epsilon-\epsilon_{\vec{n}}) = \frac{V}{4\pi^2} \left(\frac{2m}{\hbar^2}\right)^{3/2} \epsilon^{1/2}
\end{align*}


\subsubsection*{The Harmonic solid}
Consider a 1D crystal lattice with uniform lattice spacing $a$ between the points. The positions are given by $r_j = R_j + x_j$, where $R_j=a\cdot j$ is the equilibrium position (ind. of time), $x_j$ is the deviation and $j=\curly{0,1,\dots,N-1}$. Modeling it as springs, with periodic BC, we get 
\begin{align*}
    H = \frac{1}{2} m \sum_{j=0}^{N-1} \dot{r}_j^2 + \frac{K}{2} \sum_j (r_{j+1} - r_j - a)^2 = \frac{m}{2}\sum_{j=0}^{N-1} \dot{x}_j^2 + \frac{K}{2} \sum_j (x_{j+1}-x_j)^2
\end{align*}

Next, we want to transform the system such that coupling terms vanish. We introduce the Fourier transform (ommit tilde's).
\begin{align*}
    X_k = \frac{1}{\sqrt{N}} \sum_j x_j e^{-ikR_j},\quad x_j = \frac{1}{\sqrt{N}} \sum_k X_k e^{ikR_j},\quad x_j\in\mathbb{R}\implies X_k^* = X_{-k}
\end{align*}

The periodic BC yields $x_{j+N}=x_j\implies e^{ikNa}=1\implies k = \frac{2\pi}{Na}n$ for $n\in\mathbb{Z}$. Also, for $\tilde{k}=2\pi z/a$, where $z$ is an integer, $X_{\tilde{k}}=X_k$. This means that all info about $x_j$ is gotten from $X_k$ in the interval $k\in[0,2\pi/a)$ or $k\in[-\pi/a,\pi/a)$. This is the \textit{First Brillouin zone}. Customary to take both positive and negative values of $k(n)$, and we get 
\begin{align*}
    n = 0,\pm1,\pm2,\dots, \begin{cases}
        \pm(N-1)/2 \quad&\text{for }\: N\: \text{odd} \\ 
        \pm(N/2-1),N/2\quad&\text{for }\: N\: \text{even}
    \end{cases}
\end{align*}


Now, the kinetic energy can be expressed through the fourier modes, using a geometric series 
\begin{align*}
    \sum_j \dot{x}_j^2 &= \sum_j \frac{1}{N}\sum_{k,k'}\dot{X}_k \dot{X_k'} e^{i(k+k')R_j} \\ 
    \sum_{j=1}^{N-1} e^{i(k+k')R_j} &= \sum_{j=0}^{N-1}e^{i2\pi(n+n') j/N} = N\delta_{n,-n'} \\ 
    \implies \sum_j \dot{X}_j^2 &= \sum_k \dot{X}_k \dot{X}_{-k} = \sum_k \abs{\dot{X}_k}^2 
\end{align*}
Doing a similar calculation for the potential term, one obtains eventually 
\begin{align*}
    H = \frac{m}{2} \sum_k \abs{\dot{X}_k}^2 + \frac{K}{2} \sum_k 4\sin^2(ka/2) \abs{X_k}^2
\end{align*}

and we have effectively managed to decouple the system, such that $H$ is now a collection of independent harmonic oscillators with stiffness $K_k$ and frequency, $\omega_k$ with 
\begin{align*}
    K_k = 4K \sin^2(ka/2),\quad \omega_k^2 &= K_k/m = \tilde{\omega}^2 4\sin^2(ka/2) \\
    \omega_k = 2 \tilde{\omega}\abs{\sin(ka/2)}
\end{align*}

The harmonic solid is essentially a sum of indipendent QM harmonic oscillators. 

The partition function is a product over $k$, since $H$ is a sum over $k$. It is also a sum over occupation number $n$, and thus yields 
\begin{align*}
    Z &= \prod_k \sum_{n=0}^\infty e^{-\beta \hbar \omega_k (n+1/2)} \\ 
    F &= \sum_k \closed{\frac{\hbar \omega_k}{2} + k_B T \ln\closed{1-e^{-\beta \hbar \omega_k}} } \\
    U &= \sum_k \closed{\frac{\hbar \omega_k}{2} + \frac{\hbar \omega_k}{e^{\beta \hbar \omega_k} - 1} } \\ 
    \text{large}\: N \implies U &= \frac{Na}{2\pi} \int_{-\pi/a}^{\pi/a} dk\, \closed{\frac{1}{2}\hbar \omega_k + \frac{\hbar \omega_k}{e^{\beta \hbar \omega_k} - 1} } \\ 
    \text{high}\: T \implies U &\approx \frac{Na}{2\pi} \int_{-\pi/a}^{\pi/a} dk\, \closed{\frac{1}{2}\hbar \omega_k + k_B T} \\ 
    \text{high}\: T \implies c_V &= k_B
\end{align*}
So for large $T$ the specific heat gives a factor $k_B$ per particle. For 3D it would be $c_V=3k_B$ per particle. 

\subsubsection*{Debye approximation}
For $T\to0$ then $\beta\to\infty$ which kills the second term in the $U$ integral. However, at certain $k$ we have $\omega_k\to0$, so we must be careful. At low temperatures only the low-energy modes will be significantly excited, as the higher ones are cut off by the exponential term.  

For small values of $k$, we have $\omega_k\approx \tilde{\omega} ka$ which is linear in $k$. The \textit{Debye approximation} gives the linear relation as $\hbar\omega_k=\hbar v \abs{\vec{k}}$, where $v$ is the speed of sound. The Debye approximation can be viewed as an interpolation between known regions of high and low $T$. 

We now consider 3D, and approximate the first Brillouin zone by a sphere of radius $k_D$ such that it contains $3N$ modes. Considering a sphere in $n$ space, we have $n_x,n_y,n_z$ positive, and each have three modes (two transversal one longitudinal) such that 
\begin{align*}
    3N&=\frac{3}{8}\int_0^{n_D} 4\pi n^2 dn \implies n_D = (6N/\pi)^{1/3} \\ 
    k_D \pi n/L &= \closed{\frac{6\pi^2}{a^3}}\quad \text{No N in Olav's calculation}
\end{align*}
The Debye energy is then $\hbar \omega_D\equiv \hbar v k_D$. Define the \textit{Debye temperature} $\theta_D\equiv \hbar \omega_D/k_B$. Using spherical coordinates and then setting $x=\beta\hbar v k$, we get 
\begin{align*}
    U &= \text{const} + 3N\closed{\frac{a}{2\pi}}^3 \int_{\text{1st B.Z}} d^3 k\,\frac{\hbar v \abs{\vec{k}}}{e^{\beta\hbar v \abs{\vec{k}}} - 1} \\ 
    &= \text{const} + 3N\closed{\frac{a k_D}{2\pi}}^3 4\pi \frac{k_B T}{(\beta\hbar vk_D)^3} \int_0^{\theta_D / T} dx\, \frac{x^3}{e^x - 1} \\ 
    &= \text{const} + 9N \frac{k_B T}{(\theta_D / T)^3} \int_0^{\theta_D / T} dx\, \frac{x^3}{e^x - 1}
\end{align*}
For high $T\:(T\gg\theta_D)$ we have $x<1$ and we get 
\begin{align*}
    U &= \text{const} + 9N \frac{k_B T}{(\theta_D / T)^3} \int_0^{\theta_D / T} dx\, x^2 \\
    &= \text{const} + 3N k_B T
\end{align*}
At low $T\:(T\ll\theta_D)$ we can approximate the upper limit of the integral as $\theta_D/T\to\infty$ where the integral becomes $\pi^4/15$. Thus 
\begin{align*}
    U &= \text{const} + \frac{3N\pi^4}{5} \frac{(k_B T)^4}{(\hbar\omega_D)^3} \\ 
    c_V &= \frac{12}{5}\pi^4 k_B \closed{\frac{k_B T}{\hbar\omega_D}} \propto T^3 
\end{align*}
which is the same $T$ dependency as for insulators, and is observed to be correct for insulating crystals. 

\textbf{Summary of the Debye approximation:}
The steps of the Debye approximation is  
\begin{enumerate}
    \item Replace true energy spectrum with an approximate one that is linear in wave number $k$, and spherical symmetric, $\eps(\vec{k})=\hbar v \abs{\vec{k}}$.
    \item Replace true B.Z. by a spherically symmetric one. 
    \item Choose size of spherically symmetric B.Z do that it contains exactly $N$ values of $k$ and $3N$ modes. 
\end{enumerate} 
The approximation is axcellent at both high and low $T$, but is only mediocre in between. Specific heat from lattice vibrations in real crystals is monotoically increasing with temperature with a $T^3$-dependence at low temperatures to a constant value of $3k_B$ at high temperatures (Dulong and Petit) 

\subsubsection*{\scriptsize Grand Canonical Ensemble}
In equilibrium with a reservoir. Can exchange energy and particles. 
\begin{align*}
    P(E,N) &= \frac{1}{\mathcal{Z}}\Omega(E,V,N) \exp[-\beta E + \beta \mu N], \\
    \mathcal{Z} &= \sum_{N=0}^\infty \sum_E \Omega(E,V,N)\exp[-\beta E + \beta \mu N]
\end{align*}

Denote a quantum number by $\alpha$, with the corresponding occupation number $n_\alpha$. For e.g. electrons, the occupation numbers are $n_\alpha=0,1$, since each state can be occupied by one electron at max. Then, we get 
\begin{align*}
    \mathcal{Z} = \sum_{\{n_\epsilon\}} \prod_\epsilon e^{-\beta(\epsilon-\mu)n_\epsilon} = \prod_\epsilon \sum_{n_\epsilon} e^{-\beta(\epsilon-\mu)n_\epsilon} = \prod_\epsilon \mathcal{Z}_\epsilon
\end{align*} 


For an extensive system we then have 
\begin{align*}
    \ln\mathcal{Z} = \beta PV 
\end{align*}

The average values at a particular energy are now given by 
\begin{align*}
    \expval{n_\epsilon} &= \frac{1}{\mathcal{Z}_\epsilon} \sum_{n_\epsilon} n_\epsilon e^{-\beta(\epsilon-\mu)n_\epsilon} \\ 
    \expval{N} = \sum_\epsilon \expval{n_\epsilon} \\ 
    U &= \expval{E} = \sum_\eps \eps_\alpha \expval{n_\eps}
\end{align*}

\subsubsection*{\scriptsize Bosons and Fermions}
For bosons, we have $n_\eps\in\{0,\infty\}$. By using both the geometric sum of the single-energy partition function, and differentiating it, we can express $\expval{n_\eps}$. For fermions we have $n_\eps=0,1$, so we get 
\begin{align*}
    \expval{n_\eps} = \frac{1}{e^{\beta(\eps-\mu)} \pm 1}
\end{align*}
where $+=$fermions, and $-=$bosons. For the grand partition function we get 
\begin{align*}
    \mathcal{Z} = \begin{cases}
        \prod_\eps (1+\exp(-\beta(\eps-\mu))),\quad&\textbf{fermions} \\ 
        \prod_\eps {1-\exp[-\beta(\eps-\mu)]}^{-1},\quad&\textbf{bosons}
    \end{cases}
\end{align*}

The logarithm is then 
\begin{align*}
    \mathcal{Z} &= \pm \sum_\eps \ln(1 \pm \exp[-\beta(\eps-\mu)]) \\ 
    &= \pm \int_0^\infty d\eps\, D(\eps) \ln(1 \pm \exp[-\beta(\eps-\mu)]) = \beta PV
\end{align*}
The integral expressions for $N$ and $U$ are then 
\begin{align*}
    N &= \int_0^\infty d\eps\,D(\eps) (\exp[\beta(\eps-\mu)]\pm1)^{-1} \\ 
    U &= \int_0^\infty d\eps\, \eps D(\eps) (\exp[\beta(\eps-\mu)]\pm1)^{-1}
\end{align*}


\subsubsection*{Bose-Einstein statistics}
Since $\expval{n_\eps}>0$, for bosons we must have $\eps>\mu$. Setting $\eps=0$ as the lowest energy state, we have $\mu<0$. At low-T, we find the integral expression for $N$ with the substitution $x=\beta\eps$, writing $D(\eps)=\chi \eps^{1/2}$ and $e^{\beta\mu}=\lambda$ 
\begin{align*}
    N = \chi (k_B T)^{3/2} \int_0^\infty dx\, \frac{x^{1/2}}{\lambda^{-1} e^x - 1}
\end{align*}
Since $\mu<0\implies\lambda^{-1}>1$. Now, $N$ should stay constant, but decreases by a factor $T^{3/2}$ as the temperature is reduced. The integral should then be increased accordingly. The only parameter available is $\lambda$. We thus need $\lambda^{-1}$ as small as possible, but we can't decrease past $\lambda^{-1}=1$. The integral doesn't diverge, and at $\lambda=1$, we have 
\begin{align*}
    N = \chi (k_B T_E)^{3/2} 2.315 \implies k_B T_E = \left( \frac{2\pi\hbar^2}{m}\right) \left( \frac{N}{2.612V} \right)^{2/3}
\end{align*} 
$N$ not valid for $T<T_E$. The problem is due to $\eps_0=0\to\expval{n_0}=[\exp(-\beta\mu)-1]^{-1}$, which is infinite for $\mu=0$, and is arbitrarily large for small values of $\mu$. Therefore, approximating the sum as an integral doesn't work at low temperatures, since the summand is insufficiently smooth. Solved by treating the lowest energy level separate. 
\begin{align*}
    N = N_0 + \int_{\eta\to0^+}^\infty d\eps\,D(\eps) \expval{n_\eps}
\end{align*}
where the error goes to zero as $\eta\to0$. Then we get 

\begin{align*}
    N &= N_0 + N \left(\frac{T}{T_E}\right)^{3/2} \\ 
    N_0 &= [\exp(-\beta\mu)-1]^{-1} = N\bracket{1-\closed{\frac{T}{T_E}}^{3/2} }
\end{align*}
Expaning the \textit{small} $\beta \mu$ for $T<T_E$, we get 
\begin{align*}
    \mu \approx -\frac{k_B T}{N}\bracket{1-\closed{\frac{T}{T_E}}^{3/2}}^{-1}
\end{align*}


\subsubsection*{Fermi-Dirac statistics}
The occupation number is 
\begin{align*}
    f(\eps)=\frac{1}{e^{\beta(\eps-\mu)} +1} > 0
\end{align*}
and for $T\to0\; (\beta\to\infty)$ it becomes a step function with $f(\eps<\mu)=1$ and $f(\eps>\mu)=0$. 

The Fermi energy is given by $\eps_F \equiv \lim_{T\to0} \mu(T,N)$, and lies exactly in the middle between the energy of the highest occupied state and the lowest non-occupied state. The average occupation number, the \textit{Fermi function} is given by 
\begin{align*}
    f(\eps; T\to0) &= \Theta(\eps_F-\eps)
\end{align*}

Using the step function we get for $T\to0$ 
\begin{align*}
    N&=\sum_{\vec{k}} f(\eps_{\vec{k}}) = \int_0^\infty d\eps\, D(\eps)f(\eps) = \int_0^{\eps_F} d\eps\, D(\eps) = X \frac{2}{3} \eps_F^{3/2} \\ 
    U&=X \frac{2}{5} \eps_F^{5/2}
\end{align*}
From $N$, we get $X\propto V\implies \eps_F \propto(N/V)^{2/3}$. For electrons, we must multiply by a factor of $2$ to account for electron spin. 

We get the ratio $U/N=\frac{3}{5}\eps_F$. Using the Euler equation at $T=0$, we get 
\begin{align*}
    U=TS-PV+\mu N = -PV+\eps_F N \implies PV \frac{2}{5}\eps_F N 
\end{align*}
For $\eps_F=y(N/V)^{2/3}$, we find $\kappa_T^{-1}=\frac{2}{3}\eps_F \frac{N}{V}$



\subsubsection*{\scriptsize Sommerfeld expansion}
At low non-zero $T$, the temperature dependence of $N$ and $U$ is difficult to extract. We use the \textit{Sommerfeld expansion}, valid for $k_B T/\eps_F\ll1$, to overcome this with a function $\phi(\eps)$ which is $D(\eps)$ and $\eps D(\eps)$ for $N$ and $U$, respectively. The integral, with $f(\eps)$ being the Fermi function, is then 
\begin{align*}
    I = \int_0^\infty d\eps\, \phi(\eps) f(\eps)     
\end{align*}
Now, we use that  
\begin{align*}
    f(\mu+x) &= \frac{1}{e^{\beta x}+1} = 1 - \frac{1}{e^{-\beta x}+1} = 1-f(\mu-x)
\end{align*}
For $\eps<\mu$ we use the RHS, and for $\eps>\mu$ we use the LHS.
\begin{align*}
    I &= \int_0^\mu d\eps\, \phi(\eps)\bracket{1-\frac{1}{e^{-\beta(\eps-\mu)} +1 }} + \int_\mu^\infty d\eps\, \phi(\eps) \frac{1}{e^{\beta(\eps-\mu)} +1} \\ 
    &=\int_0^\mu d\eps\, \phi(\eps) - \int_0^\mu d\eps\, \phi(\eps)\frac{1}{e^{-\beta(\eps-\mu)} +1 } + \int_\mu^\infty d\eps\, \phi(\eps) \frac{1}{e^{\beta(\eps-\mu)} +1}
\end{align*}
The first term is the step function contribution, while the latter two are corrections. For the first correction term, substitute $z=-\beta(\eps-\mu)$, and for the last term $z=\beta(\eps-\mu)$. At low $T$, we may approximate the lower limit $z=\beta\mu\to\infty$. Collecting terms yields 
\begin{align*}
    I=\int_0^\mu d\eps\, \phi(\eps) + \int_0^\infty \frac{d\eps}{\beta} \frac{\phi(\mu+z/\beta) - \phi(\mu-z/\beta)}{1+e^z} 
\end{align*}

Taylor expanding both $\phi$ functions gives eventually 
\begin{align*}
    \phi(\mu+z/\beta) - \phi(\mu-z/\beta) = \frac{2z}{\beta}\phi'(\mu) + \frac{2}{3!}\closed{\frac{z}{\beta}}^3 \phi'''(\mu)+...
\end{align*} 

Then the integral becomes 
\begin{align*}
    I &= \int_0^\mu d\eps\, \phi(\eps) + (k_B T)^2 \phi'(\mu) 2 \int_{0}^\infty dz\, \frac{z}{e^z+1} + (k_B T)^4 \phi'''(\mu) \frac{2}{3!} \int_0^\infty dz\, \frac{z^3}{e^z+1} \\ 
    &= \int_0^\mu d\eps\, \phi(\eps) + (k_B T)^2 \phi'(\mu) \frac{\pi^2}{6} + (k_B T)^4 \phi'''(\mu) 7 \frac{\pi^4}{360} + \mathcal{O}(T^6)
\end{align*}

\textbf{Heat capacity at low T:} We then have 
\begin{align*}
    \phi(\eps)=X\eps^{3/2}\implies U=X[2/5 \mu^{5/2} + \pi^2/4 (k_B T)^2 \mu^{1/2}] + \mathcal{O}(T^4)
\end{align*}

Now we need to determine $\mu$. We know $N(T=0)$, and for increasing $T$ we want $N$ to stay constant. At $T=0$, we have $N=X\frac{2}{3}\eps_F^{3/2}$. At low $T$ we have 
\begin{align*}
    N &= X \frac{2}{3}\eps_F^{3/2} = X \frac{2}{3}\mu^{3/2} + X\frac{\pi^2}{12} (k_B T)^{2} \mu^{-1/2} + \mathcal{O}(T^4) \\ 
    \implies \eps_F^{3/2} &= \mu^{3/2}\bracket{1+\frac{\pi^2}{8}\closed{\frac{k_B T}{\mu}}^2 } + \mathcal{O}(T^4)
\end{align*}
$(k_B T/\mu)^2$ is small and can be replaced by $(k_B T/\eps_F)^2$ with and error $\mathcal{O}(T^4)$, and is solved by iteration. Solving the above equation for $\mu$ and taylor expanding the term of power $(-2/3)$ yields 
\begin{align*}
    \mu \approx \eps_F \closed{1 - \frac{\pi^2}{12} \closed{\frac{k_B T}{\eps_F}}^2+...}
\end{align*}

Plugging in for $U$ and expanding $\mu^{5/2}$ and $\mu^{1/2}$ up to $T^2$ gives 
\begin{align*}
    U &= \frac{2}{5}X\eps_F^{5/2} + \frac{\pi^2}{6}(k_B T)^2 X\eps_F^{1/2} \\ 
    \implies C_V &= \frac{\pi^2}{3}k_B \closed{\frac{k_B T}{\eps_F}} X \eps_F^{3/2} \\ 
    &= \frac{\pi^2}{2} Nk_B \closed{\frac{k_B T}{\eps_F}} + \mathcal{O}(T^3)
\end{align*}
where $X\eps_F^{3/2}=3/2\cdot N $. The linear dependence is observed for metals at low $T$. 


\subsubsection*{\scriptsize Semiconductors - Fermions at low temperature}

Periodic solids have energy bands separated by energy gaps where $D(\eps_V<\eps<\eps_C)=0$, where $\eps_V$ is the upper energy limit of the \textit{valence band} (VB) and $\eps_C$ is the lower energy of the \textit{conductor band} (CB). At $T=0$ electrons fill up every state up to $\eps_F$, and where $\eps_F$ is found is strongly affecting the behavior of materials. Typical metals have the fermi energy in the CB. But, for a full VB and empty CB we need a high energy to excite electrons (Insulator or semiconductor for small gap). The specific heat is then not linear in $T$ as we found for $T=0$ in the Sommerfeld expansion. 


The number of electrons is given by 
\begin{align*}
    N &= \int_0^{\eps_V} d\eps\,D_V(\eps) f(\eps) + \int_{\eps_C}^\infty d\eps\,D_C(\eps) f(\eps) \\ 
    &= \int_0^{\eps_V} d\eps\,D_V(\eps)\quad\text{for low}\: T 
\end{align*}
since $f(\eps<\mu)=1$ and $f(\eps>\mu)=0$ as $T\to 0$. Want the same $N$ as we increase $T$. Subtracting the two terms yield 
\begin{align*}
    0 &= \int_0^{\eps_V} d\eps\,D_V(\eps) [f(\eps)-1] + \int_{\eps_C}^\infty d\eps\,D_C(\eps) f(\eps)
\end{align*}

At small, finite $T$, assuming $\mu$ in the middle of the gap gives $f(\eps)-1=-1/(e^{-\beta(\eps-\mu)}+1)\approx-e^{\beta(\eps-\mu)}$ for $\eps<\eps_V$ and $f(\eps)\approx e^{-\beta(\eps-\mu)}$ for $\eps>\eps_C$. Setting $x=\beta(\eps-\eps_C)$ and $y=\beta(\eps_V-\eps)$ gives 
\begin{align*}
    0 &= \int_{\eps_C}^\infty d\eps\,D_C(\eps) e^{-\beta(\eps-\mu)} - \int_0^{\eps_V} d\eps\,D_V(\eps) e^{\beta(\eps-\mu)} \\
    &= \int_0^\infty \frac{dx}{\beta} D_C(x/\beta+\eps_C) e^{\beta(\mu-\eps_C)-x} + \int_{\beta\eps_V\to\infty}^0 \frac{dy}{\beta} D_V(\eps_V-y/\beta) e^{\beta(\eps_V-\mu)-y} 
\end{align*}
With a factor $e^{\beta\mu}$ in both integrals, we get the ratio 
\begin{align*}
    e^{2\beta\mu} &= e^{\beta(\eps_V+\eps_C)} \bracket{\frac{\int_0^\infty \frac{dy}{\beta} D_V(\eps_V-y/\beta) e^{-y} }{\int_0^\infty \frac{dx}{\beta} D_C(x/\beta+\eps_C) e^{-x}}}
\end{align*}
Taking the log to obtain $\mu$ expression, we assume $D_C(\eps)=A(\eps-\eps_C)^a$ for $\eps>\eps_C$ and $D_V(\eps)=B(\eps_V-\eps)^b$ for $\eps<\eps_V$. Then, 
\begin{align*}
    \mu &= \frac{\eps_V+\eps_C}{2} + \frac{1}{2\beta} \ln((k_B T)^{b-a} X) \\
    &= \frac{\eps_V+\eps_C}{2} + \frac{b-a}{2} k_B T \ln k_B T +\frac{1}{2}k_B T \ln X
\end{align*}
where X is the remaining integral expression that is independent of $T$. For $a=b$ we get 
\begin{align*}
    \mu = \frac{\eps_V+\eps_C}{2} + \frac{1}{2}k_B T \ln(B/A)
\end{align*}
which is linear in $T$. 

Using the expression for $\mu$ with $A\sim B$ at low $T$ one find $N_C \sim e^{-\beta/2 (\eps_C-\eps_V)}$ and is exponentially suppressed at low $T$. 


\subsubsection*{\tiny Extrinsic Semiconductors (adding dopants)}
Add impurity in the gap just below $\eps_C$. Two kind of states:
\begin{itemize}
    \item Bond state: b,s = band number and $\vec{k}$, $\curly{\uparrow,\downarrow}$  
    \item Donor state: d,s = where the state is, $\curly{\uparrow,\downarrow}$
\end{itemize}
Cost much energy to have both up and down in the same donor state (Coulomb). Ignore Coulomb in overall model, but ommit two spins in a single donor state. The partition function is $\Z=\Z_b \Z_d$. For $\Z_b$ we use $\eps_{b,s}=\eps_b$

\begin{align*}
    \Z_b &= \prod_b \closed{\sum_{n_{b,\uparrow}} e^{-\beta(\eps_b-\mu)n_{b,\uparrow}} } \closed{\sum_{n_{b,\downarrow}} e^{-\beta(\eps_b-\mu)n_{b,\downarrow}} } \\ 
    &= \prod_b \closed{1 + 2 e^{-\beta(\eps_b-\mu)} + e^{-2\beta(\eps_b-\mu)}}
\end{align*}

For donor levels, we get 
\begin{align*}
    \Z_d &= \prod_d \Z_{d,1} =  \prod_d (1+2\exp(-\beta\eps_d - \mu)) \\ 
    n_d &= \frac{1}{\frac{1}{2}e^{\beta(\eps_d-\mu)}+1}
\end{align*} 
The factor $1/2$ in $n_d$ for donor levels has given the name \textit{semiconductor statistics} to the occupation number. 



\subsubsection*{\boxed{\text{Ising model}}}
Ising model is a proposed theoretical model for magnetism, which is given by 
\begin{align*}
    H = -J \sum_{\langle{i,j}\rangle} \sigma_i \sigma_j - h \sum_i \sigma_i 
\end{align*}
where $J$ is the \textit{exchange parameter}, due to a derivation from quantum properties of exchangig electrons between neighbors. $h$ is the magnetic field, $\sigma_i=\pm1$ is the "spin", and the summation index $\langle i,j \rangle$ indicates that the sum is over nearest neighbor pairs of sites. 

\subsubsection*{\scriptsize Ising chain (1D)}
Using $h=0$, we get 
\begin{align*}
    H = -J\sum_{i=1}^{N-1} \sigma_i \sigma_{i+1}
\end{align*}

With open boundary conditions, we now define $\tau_1=\sigma_1,\,\tau_i=\sigma_{i-1}\sigma_i$, where $\tau_i=\pm1$. Thus, the Hamiltonian becomes $H=-J\sum_{i=2}^N \tau_i$. The Hamiltonian is independent of $\tau_1$, which must also be summed over, which essentially accounts for the fact that the Hamiltonian is invariant upon flipping all the spins 
\begin{align*}
    Z &= \sum_{\tau_1} \prod_{i=2}^N \closed{\sum_{\tau_i} e^{\beta J \tau_i}} = 2(2\cosh(\beta J))^{N-1} \\ 
    U &= \pdv{(\beta F)}{\beta} = -(N-1)J\tanh(\beta J) \\ 
    c &= \frac{1}{N}\pdv{U}{T} = k\beta^2 J^2 \closed{1-\frac{1}{N}} \frac{1}{\cosh^2(\beta J)}
\end{align*} 


\subsubsection*{\scriptsize Ising chain with transfer matrices}
With $J\neq0$ and $h\neq0$, we now consider periodic boundary conditions with $\sigma_{N+1}=\sigma_1$. We write the Hamiltonian in a symmetric way 
\begin{align*}
    H &= -J\sum_{i=1}^N \sigma_i \sigma_{i+1} - \frac{h}{2}\sum_{i=1}^N (\sigma_i+\sigma_{i+1}) \\
    Z &= \sum_{\curly{\sigma}} \prod_{i=1}^N e^{\bracket{\beta J \sigma_i \sigma_{i+1} + \frac{\beta h}{2}(\sigma_i + \sigma_{i+1}) }} = \sum_{\curly{\sigma}} \prod_{i=1}^N T_{\sigma_i,\sigma_{i+1}}
\end{align*}
Where $T$ is a transfer matrix, given as 
\begin{align*}
    T = \begin{pmatrix}
        e^{\beta J-\beta h} & e^{-\beta J} \\ 
        e^{-\beta J} & e^{\beta J + \beta h}
    \end{pmatrix}
\end{align*}
with the rows governed by $\sigma_i=-1,+1$ and columns by $\sigma_{i+1}=-1,+1$ respectively. Thus, 
\begin{align*}
    Z = \sum_{\curly{\sigma}} T_{\sigma_1,\sigma_2} T_{\sigma_2,\sigma_3},\dots,T_{\sigma_N,\sigma_1} = \Tr\closed{T^N}
\end{align*}
To solve the trace, we diagonalize the transfer matrix, $T=R^{-1}DR$, and use the cyclic properties of traces, to obtain $Z=\Tr(D^N)=\lambda_1^N + \lambda_2^N$. For large $N$, we let $\lambda_1>\lambda_2$ and get $Z = \lambda_1^N$
\begin{align*}
    \lambda_1 &= e^{\beta J}\cosh(\beta J) + \sqrt{e^{2\beta J}\cosh^2(\beta h)-2\sinh(2\beta J) } \\ 
    \frac{F}{N} &=-k_B T \ln\lambda_1
\end{align*}
for $h=0$, we get $F/N=-k_B T \ln(2\cosh(\beta J))$. There is no phase transition in $1D$. 

The magnetization, $m$, and magnetic susceptibility, $\chi$, is given by 
\begin{align*}
    m &= \frac{1}{N} \expval{\sigma_j} = \frac{1}{\beta N} \pdv{h} \frac{1}{Z}\sum_{\curly{\sigma}} e^{\beta J \sum_i \sigma_i \sigma_{i+1} + \beta h \sum_i \sigma_i} \\ 
    &= \frac{1}{\beta N} \pdv{\ln Z}{h} = -\frac{1}{N}\pdv{F}{h} \\ 
    \chi &= \pdv{m}{h} = \frac{1}{\beta N} \pdv[2]{\ln Z}{h}
\end{align*}



\subsubsection*{Mean Field Approximation}
Approximate interactions on a spin from its neighbors as an average magnetization. We have either $J>0$ favoring alignment of spins (ferromagnet), or $J<0$ favoring anti-alignment. We choose $J>0$ where we expect disorder at high $T$ and order at low $T$. For the mean field approximation, using the average mangetization per site, $m=1/N \sum_j \expval{\sigma_j}$. With our approximation we assume that all sites see the same average magnetization, so $\expval{m}=m_j$. we define the deviation from the average $\tilde{\sigma}_j=\sigma_j-m$. The interaction term now becomes 
\begin{align*}
    \sigma_j\sigma_k = m^2 + m(\tilde{\sigma}_j + \tilde{\sigma}_k) + \tilde{\sigma}_j \tilde{\sigma}_k \approx m^2 + m(\tilde{\sigma}_j + \tilde{\sigma}_k)
\end{align*}
where we in the \textit{mean field approximation} assume a small deviation, such that the cross term is neglected. Inserting the original terms again, we have $\sigma_j\sigma_k=-m^2+m(\sigma_j+\sigma_k)$. Inserting into the Hamiltonian, we include a factor $1/2$ for the double counting of bonds, and sum over $\sigma_j \sigma_{j+\delta}$, where $\delta$ is the neighbors of $j$, which for the 2D case is $\delta=\pm x,\pm y$.  We introduce the coordination number, $z=\sum_\delta 1$, as the number of neighbors ($z=4$ for the square lattice). Neglecting $h$ for now, the Hamiltonian becomes 
\begin{align*}
    H &= -\frac{J}{2}\sum_j \sum_\delta \closed{-m^2 + m\sigma_j + m\sigma_{j+\delta}} = Jm^2 \frac{Nz}{2} - \frac{J}{2} mz \sum_j \sigma_j - \frac{J}{2}m \sum_j \sum_\delta \sigma_{j+\delta} \\ 
    &= Jm^2 \frac{Nz}{2} - J mz \sum_j \sigma_j
\end{align*}  
where the final term comes from shifting $j\to j'=j+\delta$ since we have periodic BC. 

Adding the field again, we get 
\begin{align*}
    H_{\mathrm{MFA}} &= Jm^2 \frac{Nz}{2} - (h+Jmz)\sum_j \sigma_j = Jm^2 \frac{Nz}{2} - h_{\mathrm{eff}} 
\end{align*}


The partition function now becomes 
\begin{align*}
    Z &= e^{-\beta J m^2 Nz/2} \sum_{\curly{\sigma}} e^{\beta h_{\mathrm{eff}}\sum_j \sigma_j } = e^{-\beta J m^2 Nz/2} \closed{\sum_{\sigma_1} e^{\beta h_{\mathrm{eff}} \sigma_1 } }^N \\ 
    &= e^{-\beta J m^2 Nz/2} \closed{2\cosh(\beta h_{\mathrm{eff}})}^N = e^{-\beta J m^2 Nz/2} Z_1^N
\end{align*}


Now, we can calculate the average magnetization, remembering that $m=m_j\implies m=m_1=\expval{\sigma_1}$, so we get 
\begin{align*}
    m &= \frac{1}{Z_1} \sum_{\sigma_1=-1}^{+1} \sigma_1 e^{(\beta h_{\mathrm{eff}}\sigma_1)} = \tanh(\beta h_{\mathrm{eff}}) \\ 
    &= \tanh(\beta h + \beta Jzm)
\end{align*}
where the last equality is the MFA self-consistent equation for $m$, but there is no closed form solution for it. For $h=0$, set $x=\beta J zm\implies \frac{k_B T}{Jz}x=\tanh(x)$. For disordered we may have the LHS bigger than $1$, which means that $m=0$. For the ordered phase, $k_B T/Jz < 1$, there are $3$ solutions, $x=\pm1,0$, but $x=0$ turns out to be unstable. The critical temperature is $k_B T_c=zJ=4J$ for the square lattice. The exact value, however is $k_B T_c\approx 2.26 J$.   

The unstable solution of $m=0$ in the ordered phase is due to the fact that $F$ must be minimized to be stable, while $k_B T/Jz < 1$ corresponds to a maxima of $F$ for $m=0$.  


For $T\lesssim T_c$ with $h=0$, $m$ is small, and we can expand to solve for $m$
\begin{align*}
    m \approx \beta J m z - \frac{1}{3} (\beta m J z)^3 \implies m^2 = 3 \closed{\frac{T}{T_c}}^2 \closed{1-\frac{T}{T_c}} 
\end{align*}
where we have used that $Jz=k_B T_c$. Near $T_c$, the squared term is negligible, and we get 
\begin{align*}
    m \propto \closed{\frac{T_c-T}{T_c}}^{1/2}
\end{align*}

We always have $m\propto(...)^{\beta}$, and for MFA $\beta=1/2$, while the 2D Ising model has $\beta=1/8$ in reality. 

\


\subsubsection*{\scriptsize Phase Transitions (PT) - Generalities}
\textbf{Definitions:}
\begin{itemize}
    \item \textbf{Phase:} Region in phase space where free energy is analytic (function of its arguments)
    \item \textbf{Phase transition:} Non-analytic
    \item \textbf{First order PT:} The derivative of the free energy is discontinuous. 
    \item \textbf{Continuous PT:} All first derivatives of free energy is continuous, but one or more higher order derivatives are discontinuous. 
\end{itemize}

\subsubsection*{\tiny Existence of PT} 

\textbf{Energy/Entropy argument:} THe partition function is $Z=\exp(-\beta F)=\sum_s \exp[-\beta H(s)]$, and at low $T$, $Z$ is dominated by the lowest energy configurations. Assume there are $N_g$ such states with energy $E_g$. This yields $Z\approx N_g \exp(-\beta E_g)=\exp[-\beta(E_g - k_B T \ln N_g)]$. Approximating the entropy as $S\approx k_B T \ln N_g$ we get $F_g(\text{low}-T)\approx E_g - k_B T \ln N_g$. $N_g$ is usually small. Excited states have $E>E_g$ and are not favored at low $T$, unless there are considerably many of them, such that their entropy becomes significant. Denote excitation by $E_{\mathrm{ex}}$, which is not much larger than $E_g$, with $N_\mathrm{ex}$ so that $F_\mathrm{ex}=E_\mathrm{ex}-k_B T \ln N_\mathrm{ex}$. We expect a PT when $F_g=F_\mathrm{ex}$. For small $\ln N_g$ we then get 
\begin{align*}
    k_B T_c \approx \frac{\Delta E}{\ln N_\mathrm{ex}}
\end{align*} 
We expect excited states to dominate $Z$ for $T>T_c$, and $E_g$ states to dominate for $T<T_c$. 

For the 1D ising model of $N$ spins with open boundary conditions and no applied magnetic field, there are $N_g=2$ ground states (all up or all down), with $E_g=-J(N-1)$. The first excited states is with approximately half of spins up and the other half down, with $E_\mathrm{ex}=E_g+2J$ and $N_\mathrm{ex}=N-1$. We then get 
\begin{align*}
    k_B T_c = 2J/(\ln(N-1)-\ln 2) \to 0,\quad\text{for}\: N\to\infty
\end{align*}
Hence, no PT at finite $T$ in 1D Ising. 

For the 2D Ising model, we have $N_g=2$ again. The energy difference is $\Delta E=2J\cdot l$, where $l$ is the circumference of the domain enclosing a collection of oppositely directed spins. The number of excited states are given by the number of ways of doing a random walk on the lattice with $l$ steps that encloses a domain, and is approximately $N_\mathrm{ex}\approx N(z-1)^l \cdot 2$, where $N$ contribution is from the number of starting points, and $z=4$ is the number of neighbors in 2D lattice. Thus,
\begin{align*}
    k_B T_c = \frac{2J\cdot l}{\ln(2N(z-1)^l)-\ln2} = \frac{2J \cdot l}{\ln N + l\cdot \ln3}
\end{align*}
For large $N$ there are also large $l$-s dominating the domain, so that 
\begin{align*}
    k_B T_c\sim \frac{2Jl}{l\ln(z-1)}=\frac{2J}{\ln3}=1.82 J
\end{align*}
the exact value for 2D Ising is $k_B T_c\approx 2.269J$.

\subsubsection*{\tiny Landau argument for PT existence}
A symmetry can't be continuously deformed into another symmetry. Thus: Two phases with different symmetries are always separated by one or more PT's (There can still be PT between phases of same symmetries).




GENEREAL PT 
CRITICAL EXPONENTS 

RENOM + SCALING 


CUMULANT EXPANSION 
CLUSTER EXPANSION 
VIRIAL EXPANSION 


DIFFUSION EQ. RW
MARKOV 


\subsubsection*{\scriptsize Random Walks}
Fixed length symmetric RW in 1D. The position after $i+1$ steps is 
\begin{align*}
    x_{i+1} = x_i + l\cdot s 
\end{align*}
where $s=\pm1$ with prob. $p=1/2$ for each, and $l$ is the step length. It follows that $\expval{x_{i+1}}=\expval{x_i}\implies \expval{x_N}=\expval{x_0}$, since we have symmetric walk. However, the expected value of $X_N^2$ gives a measure of the elapsed length, and one finds $\expval{x_{i+1}^2}=\expval{x_i^2}+l^2 \implies \expval{x_N^2}=\expval{x_0^2}+Nl^2$, so for $x_0=0$, we get $\sqrt{\expval{x_N^2}}=l\sqrt(N)$. 

With a probability $P$ and $(1-P)$ of moving to the right and left, respectively, the position after $N=L+R$ steps is $x_N=(R-L)l = (2R-N)l$, with a probability 
\begin{align*}
    P_N(R) = \binom{N}{R} P^R (1-P)^{N-R}
\end{align*}

So we have a binomial probability distribution. Some properties of this distribution are  
\begin{align*}
    \sum_{R=0}^N P_N(R) &= 1 \\ 
    \expval{R} &= \sum_R R\cdot P_N(R)=NP \\ 
    \expval{R^2} &= \sum_R R^2 P_N(R) = NP(1-p) + N^2 P^2 \\
    \sigma^2 &= P(1-P)N \\ 
    p_N(R) &\approx \frac{1}{\sqrt{2\pi P(1-P)N}}\exp\bracket{-\frac{(R-PN)^2}{2P(1-P)N}}
\end{align*}
so the binomial distribution can be approximated by a Gaussian for sufficiently large $R$ and $N$. 

This gives $\expval{X_N}=Nl(2P-1)$ and $\expval{x_N^2}=4l^2 NP(1-P) + \expval{x_N}^2$ so we get $\expval{x_N^2} - \expval{x_N}^2 = 4l^2 N P(1-P)$.

Now, for $R,N$ large, with $P=1/2$ once again, we can approximate with a Gaussian. Introduce probability density $P_N(x)\equiv P(x)/2l$, since $P(x)$ is the Gaussian distribution for discrete $x$. Then, for $t=N\Delta t$ we get 
\begin{align*}
    P_N(x) &= \frac{1}{\sqrt{2\pi Nl^2}} e^{-\frac{x^2}{2Nl^2}} \\ 
    &= \frac{1}{\sqrt{2\pi \frac{l^2}{\Delta t} t}} \exp[-\frac{x^2}{2\frac{l^2}{\Delta t} t}]
\end{align*}  
From microscopic, we define the \textit{Diffusion constant}, $D\equiv \frac{l^2}{2\Delta t}$. Thus, we get 
\begin{align*}
    P_N(x) &= \frac{1}{\sqrt{2\pi 2Dt}} e^{-\frac{x^2}{2\cdot 2Dt}}
\end{align*}
and we get a Gaussian distribution with mean $0$ and variance $2Dt$. With $M$ random walkers (particles), all with $x(t=0)=0$, the density of walkers per unit length at time $t$, $\rho(x,t)$, fulfills the Diffusion equation 
\begin{align*}
    \rho(x,t) &= M p(x,t) \\ 
    \pdv{\rho(x,t)}{t} &= D \pdv[2]{\rho(x,t)}{x}
\end{align*}


This result holds for any RW with a symmetric step distribution. Consider general RW with $x(t+\Delta t)=x(t)+l$, where $l$ is now a random length (variable) with a probability distribution $\chi(l)$ that is independent of $t$. The distribution is normalized and symmetric, with $\int dl\,\chi(l)\cdot l^2=a^2$, where the limits are $l=\pm\infty$. For fixed $l$ we previously had $\chi(l)=\frac{1}{2}(\delta(l-a) + \delta(l+a))$. 

Now, what is the distribution $p(x,t+\Delta t)$, given $p(x,t)$. Found by summing all possible paths from $x-l$ up to $x$ between the two times, weighted with the probability of the $l$'s. 
\begin{align*}
    p(x,t+\Delta t) &= \int_{-\infty}^\infty dl\, p(x-l,t)\chi(l) \\ 
    p(x-l,t) &= p(x,t) - l \pdv{p(x,t)}{x} + \frac{l^2}{2!}\pdv[2]{p(x,t)}{x} + \dots \\ 
    \implies p(x,t+\Delta t) &= p(x,t)\int_{-\infty}^\infty dl\,\chi(l) - \pdv{p(x,t)}{x} \cdot 0 + \frac{1}{2} \pdv[2]{p(x,t)}{x} \int_{-\infty}^\infty dl\,\chi(l)\cdot l^2 \\ 
    &= p(x,t) + \frac{a^2}{2} \pdv[2]{p(x,t)}{x}
\end{align*}
Moving $p(x,t)$ to the LHS and dividing by $\Delta t$ yields 
\begin{align*}
    \frac{p(x,t+\Delta t) - p(x,t)}{\Delta t} &= \frac{a^2}{2\Delta t} \pdv[2]{p}{x} \\ 
    \lim \Delta t\to 0 \implies \pdv{p}{t} &= D \pdv[2]{p}{x}
\end{align*}

\end{multicols*}
\end{document}